{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0c7eaa",
   "metadata": {},
   "source": [
    "## 딥러닝\n",
    "\n",
    "### 심층신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29fa1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로드\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f15615",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069e025",
   "metadata": {},
   "source": [
    "- 이미 이전 장에서 다우로드 했기 때문에 다시 다운로드 하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e387f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_input.reshape(-1, 28*28)\n",
    "test_input = test_input.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a887a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링\n",
    "sclaer = StandardScaler()\n",
    "train_scaled = sclaer.fit_transform(train_input.astype(np.float64))\n",
    "test_sclaed = sclaer.transform(test_input.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e4e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련세트, 검증세트 분리\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled, train_target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a891a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (12000, 784) (10000, 784)\n",
      "(48000,) (12000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_scaled.shape, val_scaled.shape, test_sclaed.shape)\n",
    "print(train_target.shape, val_target.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a0428",
   "metadata": {},
   "source": [
    "#### 심층신경망 만들기1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 밀집층 생성\n",
    "dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))\n",
    "dense2 = keras.layers.Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f840c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 생성\n",
    "model = keras.Sequential([dense1, dense2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f133695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a5f35",
   "metadata": {},
   "source": [
    "#### 심층신경망 만들기2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "838b7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),\n",
    "    keras.layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d48ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32ee68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics='accurary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "176aa616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1496/1500 [============================>.] - ETA: 0s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 훈련\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1819\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1817\u001b[39m logs = tf_utils.sync_to_numpy_or_python_type(logs)\n\u001b[32m   1818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1820\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnexpected result of `train_function` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1821\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Empty logs). This could be due to issues in input \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1822\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpipeline that resulted in an empty dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1823\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOtherwise, please use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1824\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1825\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1826\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minformation of where went wrong, or file a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33missue/bug to `tf.keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1828\u001b[39m     )\n\u001b[32m   1829\u001b[39m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[32m   1830\u001b[39m logs = \u001b[38;5;28mself\u001b[39m._validate_and_get_metrics_result(logs)\n",
      "\u001b[31mValueError\u001b[39m: Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "# 훈련\n",
    "model.fit(train_scaled, train_target, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b00aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1920, in test_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\__autograph_generated_file48_mfu6v.py:15\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     do_return = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(\u001b[38;5;28mself\u001b[39m), ag__.ld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     17\u001b[39m     do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: in user code:\n\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1920, in test_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)\n",
    "# 검증 정확도: 87.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1791ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 913us/step\n"
     ]
    }
   ],
   "source": [
    "pred_result = model.predict(test_sclaed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caa2f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "764e99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글로 Matplotlib 사용시 항상 필요\n",
    "from matplotlib import rcParams, font_manager, rc\n",
    "\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font='Malgun Gothic', rc={'axes.unicode_minus': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "002bce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['티셔츠', '바지', '스웨터', '드레스', '코트', '샌달', '셔츠', '스니커즈', '가방', '앵클부츠']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cca72d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트이미지와 예측결과 시각화\n",
    "def show_image(index):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(test_input[index].reshape(28, 28), cmap='gray_r')\n",
    "    true_label = class_names[test_target[index]]\n",
    "    pred_label = class_names[np.argmax(pred_result[index])]\n",
    "\n",
    "    # plt.title(f'실제: {true_label}, 예측: {pred_label}')\n",
    "    # plt.title('예측', fontsize=13)\n",
    "    plt.text(0, -6, f'실제: {true_label}', fontsize=12, color='blue')\n",
    "    plt.text(0, -3, f'예측: {true_label}', fontsize=12, color='red', fontweight='bold')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc24449d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i  in range(10, 14):\n",
    "    show_image(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896b959",
   "metadata": {},
   "source": [
    "#### 렐루 함수\n",
    "\n",
    "- 인공신경망 은닉층에 초기에는 Sigmoid 함수 사용\n",
    "    - 단점 : 오른쪽, 왼쪽 끝으로 갈수록 그래프가 누워 있어서 올바른 출력을 못 만듦\n",
    "\n",
    "- 렐루함수(ReLU) : 수정된 선형유닛이라는 뜻의 함수\n",
    "\n",
    "    <img src=\"../image/ml0011.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48b96a",
   "metadata": {},
   "source": [
    "#### Flatten 객체\n",
    "- 이미지 2차원 배열을 1차원으로 변경할 때 np.reshape() 사용\n",
    "- Flatten 클래스가 위에 일을 대신해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bf77345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28), name='Flatten'),\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden'),\n",
    "    keras.layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_RELU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f72a842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_RELU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7b3548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2a3a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = train_input / 255.0\n",
    "test_sclaed = test_input / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6cf24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련세트, 검증세트 분리\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled, train_target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5d835de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "model2.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4756fca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5297 - accuracy: 0.8134\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3902 - accuracy: 0.8591\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3526 - accuracy: 0.8742\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3300 - accuracy: 0.8822\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3143 - accuracy: 0.8867\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3031 - accuracy: 0.8923\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2910 - accuracy: 0.8947\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2846 - accuracy: 0.8990\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2763 - accuracy: 0.9022\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2693 - accuracy: 0.9035\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2648 - accuracy: 0.9071\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2587 - accuracy: 0.9093\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2524 - accuracy: 0.9120\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2489 - accuracy: 0.9124\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2435 - accuracy: 0.9137\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2375 - accuracy: 0.9162\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2327 - accuracy: 0.9190\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2310 - accuracy: 0.9187\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2239 - accuracy: 0.9220\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2218 - accuracy: 0.9227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297e9d354d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model2.fit(train_scaled, train_target, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ebdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 888us/step - loss: 0.4215 - accuracy: 0.8875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4215044379234314, 0.887499988079071]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(val_scaled, val_target)\n",
    "# ReLU함수 사용시, Sigmoid 함수 사용시 보다 0.01% 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd86f2",
   "metadata": {},
   "source": [
    "#### 옵티마이저\n",
    "- 하이퍼파라미터 값 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1ca9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28), name='Flatten'),\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden'),\n",
    "    keras.layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_RELU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델3 설정, 최적화는 adam 클래스만 사용할 것!\n",
    "model3.compile(loss='sparse_categorical_crossentropy', metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1822 - accuracy: 0.9327\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1761 - accuracy: 0.9348\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1719 - accuracy: 0.9356\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1696 - accuracy: 0.9357\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1646 - accuracy: 0.9392\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1624 - accuracy: 0.9387\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1559 - accuracy: 0.9416\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1536 - accuracy: 0.9428\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1505 - accuracy: 0.9433\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1468 - accuracy: 0.9456\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1452 - accuracy: 0.9456\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1400 - accuracy: 0.9478\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1400 - accuracy: 0.9474\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1369 - accuracy: 0.9481\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1339 - accuracy: 0.9505\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1296 - accuracy: 0.9509\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1281 - accuracy: 0.9531\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1271 - accuracy: 0.9528\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1242 - accuracy: 0.9537\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1211 - accuracy: 0.9553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297c6ae2ad0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델3 훈련. 모델훈련은 정보를 저장하기 때문에 반복할 수록 정확도 상승!\n",
    "model3.fit(train_scaled, train_target, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59ff5f",
   "metadata": {},
   "source": [
    "- optimizer 사용시 0.01정도 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 913us/step - loss: 0.3363 - accuracy: 0.8899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33632707595825195, 0.8899166584014893]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(val_scaled, val_target)\n",
    "# 훈련 95%, 검증 88% - 7정도 차이이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461fab2",
   "metadata": {},
   "source": [
    "#### 드롭아웃\n",
    "- 훈련과정 밀집층에 일부 뉴런을 꺼버림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25c301d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28), name='Flatten'),\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden'),\n",
    "    keras.layers.Dropout(0.3, name='dropout'), \n",
    "    keras.layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_RELU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f0c3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_RELU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8a40412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델4 설정, 최적화는 adam 클래스만 사용할 것!\n",
    "model4.compile(loss='sparse_categorical_crossentropy', metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56ab9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5845 - accuracy: 0.7956\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4376 - accuracy: 0.8429\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4037 - accuracy: 0.8538\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3839 - accuracy: 0.8596\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3653 - accuracy: 0.8666\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3524 - accuracy: 0.8708\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3433 - accuracy: 0.8744\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3327 - accuracy: 0.8765\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3253 - accuracy: 0.8800\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3182 - accuracy: 0.8814\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3110 - accuracy: 0.8837\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3128 - accuracy: 0.8844\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3014 - accuracy: 0.8870\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2963 - accuracy: 0.8894\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2940 - accuracy: 0.8903\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2874 - accuracy: 0.8916\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2852 - accuracy: 0.8930\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2820 - accuracy: 0.8944\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2795 - accuracy: 0.8951\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2748 - accuracy: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297e9d1af90>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델3 훈련. 모델훈련은 정보를 저장하기 때문에 반복할 수록 정확도 상승!\n",
    "model4.fit(train_scaled, train_target, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d5f2d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 910us/step - loss: 0.3258 - accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32580024003982544, 0.8858333230018616]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(val_scaled, val_target)\n",
    "# 훈련 89%, 검증 88% - 1차이(훈련과 검증 사이에 차이가 거의 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d87ca3",
   "metadata": {},
   "source": [
    "- 드롭아웃하면 정확도가 떨어짐 > 훈련과 검증(테스트) 사이에 정확도 창이가 줄어듬\n",
    "- `과대적합`(훈련세트 훈련에 너무 치중하여서 정확도가 너무 높은 것) 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c635e6",
   "metadata": {},
   "source": [
    "#### 모델의 저장과 복원\n",
    "- 훈련시간이 딥러닝에 크게 좌우됨. 저장이 없으면 다시 훈련시키고 시간을 소요해야 됨\n",
    "- 파일로 저장 후 모델에 대한 구조와 파라미터만 저장하는 두가지 방법이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d7ffc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델4 저장 - 파라미터만 저장\n",
    "model4.save_weights('./model4-weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b5c2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 모델4 저장 - 전체(모델 구조와 파라미터) 저장\n",
    "model4.save('./model4-whole.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439cdc1",
   "metadata": {},
   "source": [
    "##### 모델 사용\n",
    "- 파라미터만 저장한 파일을 사용하려면 먼저 모델을 생성해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b1c4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28), name='Flatten5'),\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden5'),\n",
    "    keras.layers.Dropout(0.3, name='dropout5'), \n",
    "    keras.layers.Dense(10, activation='softmax', name='output5')\n",
    "], name='Fashion_MNIST_RELU5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ad5b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.load_weights('./model4-weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "403f714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 850us/step\n"
     ]
    }
   ],
   "source": [
    "pred_result = model5.predict(test_sclaed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6d581ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_RELU5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten5 (Flatten)          (None, 784)               0         \n",
      "                                                                 \n",
      " hidden5 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dropout5 (Dropout)          (None, 100)               0         \n",
      "                                                                 \n",
      " output5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865b83a",
   "metadata": {},
   "source": [
    "- 모델까지 전부 생성해주는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78686f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = keras.models.load_model('./model4-whole.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33be5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Fashion_MNIST_RELU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc3d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 908us/step - loss: 0.3258 - accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32580024003982544, 0.8858333230018616]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.evaluate(test_scaled, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d980b5",
   "metadata": {},
   "source": [
    "#### 콜백\n",
    "- 실행도중 다른 일을 할 수 있도록 해주는 기능\n",
    "- `조기종료를` 위해 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9572f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28), name='Flatten'),\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden'),\n",
    "    keras.layers.Dropout(0.3, name='dropout'), \n",
    "    keras.layers.Dense(10, activation='softmax', name='output')\n",
    "], name='Fashion_MNIST_RELU7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "15056d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.compile(loss='sparse_categorical_crossentropy', metrics='accuracy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c3e9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크마다 모델 저장기능 콜백\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./best-model.h5', save_best_only=True)     # 최고 상태면 저장\n",
    "# 조기종료 콜백\n",
    "# 두번 이상 훈련값이 동일하면 조기종료, 이전 최고상태로 복구\n",
    "early_stopp_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d4d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5962 - accuracy: 0.7904 - val_loss: 0.4188 - val_accuracy: 0.8519\n",
      "Epoch 2/20\n",
      " 127/1500 [=>............................] - ETA: 1s - loss: 0.4364 - accuracy: 0.8445"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\iot-dataanlysis-2025\\mlvenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4423 - accuracy: 0.8417 - val_loss: 0.3856 - val_accuracy: 0.8574\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4015 - accuracy: 0.8536 - val_loss: 0.3638 - val_accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3805 - accuracy: 0.8615 - val_loss: 0.3593 - val_accuracy: 0.8655\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3678 - accuracy: 0.8651 - val_loss: 0.3471 - val_accuracy: 0.8695\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3538 - accuracy: 0.8694 - val_loss: 0.3460 - val_accuracy: 0.8742\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8740 - val_loss: 0.3406 - val_accuracy: 0.8768\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3322 - accuracy: 0.8772 - val_loss: 0.3273 - val_accuracy: 0.8806\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3275 - accuracy: 0.8786 - val_loss: 0.3316 - val_accuracy: 0.8817\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3207 - accuracy: 0.8791 - val_loss: 0.3457 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x297e976d9d0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련. 모델훈련은 정보를 저장하기 때문에 다시 수행하면 이전 정보를 담고 시작\n",
    "model7.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target),\n",
    "    callbacks=[checkpoint_cb, early_stopp_cb]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2aca650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 874us/step - loss: 0.3273 - accuracy: 0.8806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3272780478000641, 0.8805833458900452]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "65ab4749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 886us/step - loss: 0.3522 - accuracy: 0.8735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3522285223007202, 0.8734999895095825]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.evaluate(test_sclaed, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f28499f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopp_cb.stopped_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00413bf3",
   "metadata": {},
   "source": [
    "- 20번 중 15번 반복에서 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99622",
   "metadata": {},
   "source": [
    "#### 결론\n",
    "- `심층 신경망` : 2개 이상의 밀집층을 포함한 신경망 모델. 다층(심층) 인공신경망\n",
    "- `렐루함수` : 시그모이드 함수의 단점을 보안한 활성화 함수\n",
    "- `옵티마이저` : 신경망의 가중치(기울기)와 절편을 학습하기 위한 알고리즘 또는 방법. `Adam`, SGD(확률적 경사하강법), RMSprop...\n",
    "\n",
    "- `드랍아웃` : 밀집층의 뉴런을 임의로 꺼서 훈련을 덜 시키는 것 -> 과대적합을 막기위해서 수행\n",
    "- 모델 저장과 복원 : 이미 훈련된 데이터를 저장했다가 나중에 다시 쓰기 위해서\n",
    "- `콜백` : 에포크마다 모델 저장 또는 조기종료를 위해서 다른 기능을 수행하는 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
